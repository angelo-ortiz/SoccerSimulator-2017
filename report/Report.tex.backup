\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage[T1]{fontenc}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[pdftex]{graphicx}
\usepackage{comment}
\usepackage{csquotes}
\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\author{Angelo Ortiz}
\title{Rapport du Projet}

\begin{document}
\begin{titlepage}
  \centering
  \includegraphics[width=0.30\textwidth]{logo.jpg}\par\vspace{1cm}
  {\scshape\LARGE Sorbonne Universit\'e\par}
  \vspace{1cm}
  {\scshape\Large 2I013 : Projet (application)\par}
  \vspace{1.5cm}
  {\Large \bfseries Sujet :\par}
  {\huge\bfseries IA Football\par}
  \vspace{2cm}
  {\Large\itshape Fangzhou Ye\par}
  {\Large\itshape Angelo Ortiz\par}
  \vfill
  
  % Bottom of the page
  {\large Licence d'Informatique\par}
  {\large Ann\'ee 2017/2018\par}
\end{titlepage}
 
%\newpage
\tableofcontents
  
\newpage
  
\part*{Introduction}
\section*{Pr\'esentation}
Le projet a consist\'e en le d\'eveloppement d'intelligences artificielles de 
joueurs de football. 
Le simulateur du jeu \'etant d\'ej\`a fourni, notre 
travail a \'et\'e d'impl\'ementer des strat\'egies de jeu pour mieux r\'eussir 
les matches.

Lâ€™objectif du travail a \'et\'e d'apprendre \`a bien mener un projet long 
sur un nouvel environnement de d\'eveloppement, \`a savoir 
Python. 
Sachant que travail \`a \'et\'e fait en bin\^ome, on a eu 
besoin d'un outil collaboratif, en l'occurrence {\bfseries git} qui est un 
logiciel de gestion de versions d\'ecentralis\'e. 
On a ainsi appris \`a g\'erer 
l'avancement d'un projet \`a travers la m\'ethode par fonctionnalit\'es.

\section*{Aper\c{c}u}
Voici un aper\c{c}u du simulateur de football lors d'une partie de notre 
\'equipe {\itshape ChPerFusion} en rouge face \`a l'\'equipe d'un de nos 
camarades de classe.

\begin{center}
\includegraphics[width=0.8\textwidth]{apercu.png}\par%\vspace{1cm}
\end{center}

\section*{Diagrammes}
Le projet a \'et\'e organis\'e en un module {\bfseries ia} qui contient 
l'impl\'ementation de nos joueurs.

On vous pr\'esente le graphe de d\'ependences du module.

\begin{center}
  \includegraphics[width=0.8\textwidth]{packages_IA.png}\par%\vspace{1cm}
\end{center}

On vous pr\'esente \`a pr\'esent le diagramme de classes du projet.

\begin{center}
  \includegraphics[width=0.8\textwidth]{classes_IA.png}\par%\vspace{1cm}
\end{center}


\addcontentsline{toc}{part}{Introduction}
\newpage

\part{Architecture logicielle} %et demarche
Dans cette premi\`ere partie, on vous expliquera les choix faits 
concernant l'architecture logicielle.

\newpage

\part{Strat\'egies}
Dans cette deuxi\`eme partie, on vous d\'etaille le comportement de nos 
diff\'erents joueurs.
Compte tenu de la taille du terrain de jeu, le nombre maximum de joueurs par 
\'equipe est limit\'e \`a quatre. 
Ainsi, il y a trois cat\'egories de matches 
selon le nombre de joueurs par \'equipe : un, deux ou quatre.

\section{Un joueur}
Pour l'\'equipe compos\'ee uniquement d'un joueur, on a d\'evelopp\'e un seul 
joueur que l'on appelle {\itshape Fonceur}. 

\subsection*{Fonceur}
Il s'agit du point de d\'epart du projet. Le but des premi\`eres semaines 
\'etant de se familiariser avec le code source fourni, on a cr\'e\'e une 
strat\'egie tr\`es simple et na\"ive. 
Ainsi, il faisait tout au maximum : il devait se rapprocher de la balle \`a 
toute vitesse lorsqu'il ne la contr\^olait pas et il frappait avec toute sa 
puissance dans le cas contraire. 
Compte tenu que ce comportement ressemblait beacoup aux autres fonceurs de la 
classe \`a une param\'etrisation pr\`es et que les matches finissaint en nul, on 
a ajout\'e les fonctionnalit\'es suivantes : l'avanc\'ee balle au pied et le 
dribble. 
Ceci a rapport\'e de bons r\'esultats contre les \'equipes restant 
na\"ives, mais n'a pas suffit pour battre les \'equipes plus d\'evelopp\'ees. 
En effet, il est tr\`es difficile de devancer un adversaire qui fait une 
pression sur le ballon \`a une distance moyenne de sorte que les tentatives de 
dribbles du fonceur \'echouent.
De plus, ses essais de dribble dans sa propre surface de r\'eparation 
\'etaient souvent insuffisants pour \'echapper au danger que signifie 
l'adversaire venant le presser. Il d\'egage \`a pres\'ent la balle le plus loin 
possible pour pallier cette difficult\'e.

Cette strat\'egie est rest\'ee assez basique \`a l'\'egard de celles de nos 
camarades de classe vu les r\'esultats des derniers tournois hebdomadaires et 
de nos tests. Autrement dit, le dribble est tr\`es peu r\'eussi et les 
contre-attaques arrivent lorsque le joueur est dans sa course vers l'avant, ce 
qui le surprend et fait que l'adversaire se trouve facilement seul face \`a 
notre cage.

\section{Deux joueurs}
Cette cat\'egorie a \'et\'e celle sur laquelle on a consacr\'e la plupart du 
projet. On compte trois joueurs fonctionnels que l'on appelle 
{\itshape Attaquant}, {\itshape Gardien} et {\itshape D\'efenseur}. 

\subsection*{Attaquant}
Il a une vocation plut\^ot offensive. Son comportement initial \'etait celui 
d'un attaquant classique. 
Sa conduite sans la balle d\'ependait de la zone dans laquelle se trouvait le 
porteur de la balle : il le pressait si l'adversaire \'etait \'eloign\'e de sa 
surface et se d\'ecalait vers les c\^ot\'es pour faciliter les contre-attaques 
dans le cas contraire. 
Quant \`a son style de jeu avec la balle au pied, il avan\c{c}ait de petites 
distances sur le terrain et frappait la balle lorsqu'il se trouvait dans la 
surface de r\'eparation adverse. Tout comme pour le fonceur, il perdait la balle 
trop facilement. 
C'est pourquoi on a ajout\'e le dribble al\'eatoire, ce qui l'a rendu 
impr\'evisible face aux tentatives adverses de lui arracher le ballon. 
Lorsque le contr\^ole de la balle avait lieu en sa zone d\'efensive et qu'un 
adversaire \'etait si proche de lui qu'il pouvait aussi la frapper, la 
puissance de son dribble \'etait d\'epass\'ee par celle de la frappe. On a 
ajout\'e le d\'egagement vers un point quelconque tel que le tir soit le moins 
risqu\'e possible pour \'eviter ce genre de situations.

Puis on s'est rendu compte qu'il se trouvait en inf\'eriorit\'e num\'erique face 
\`a l'\'equipe adverse dans la plupart des matches. Cette contrainte a motiv\'e 
l'id\'ee de faire monter le co\'equipier dans le terrain et, par cons\'equent, 
le d\'eveloppement de l'action de passe de la balle.
Ce comportement s'est r\'ev\'el\'e tr\`es efficace en situation d'attaque vu 
que les autres \'equipes utilisent seulement un attaquant, mais les 
contre-attaques s'av\'eraient tr\`es dangereuses. Celles-ci arrivaient comme 
r\'esultat des dribbles \'echou\'es. Par ailleurs, on a remarqu\'e que les 
adversaires restaient fix\'es dans l'axe tout en laissant dans les c\^ot\'es 
des voies libres. Tout ceci nous a am\'en\'e \`a mettre en place la toute 
derni\`ere modification : le dribble lat\'eral. Dans le milieu de terrain, le 
joueur essaie syst\'ematiquement de dribbler le joueur lui bloquant la 
trajectoire vers les c\^ot\'es. Le comportement final est encore plus 
dangereux en attaque et les possibilit\'es de contre-attaques sont \`a 
pr\'esent tr\`es mitig\'ees.

\subsection*{Gardien}
La prise de d\'ecisions initiale du gardien avait pour vocation de d\'efendre 
la cage \`a tous les coups. En effet, il se positionnait dans le point central 
sur la ligne de sa cage et n'en sortait que pour d\'egager la balle lorsqu'un 
adversaire se rapprochait avec elle. 
Ce d\'egagement consistait en fait \`a faire une pseudopasse vers son 
co\'equipier, c'est-\`a-dire qu'il l'adressait vers l'un de deux points 
axialement sym\'etriques selon si le co\'equipier se trouvait en la partie haute 
ou basse du terrain. 
Le point faible de cette conduite \'etait que certains attaquants adverses 
tiraient vers notre cage depuis des longues distances et on essayait ainsi 
d'attraper la balle avec des trajectoires circulaires. On a donc d\'ecid\'e 
d'am\'eliorer la trajectoire d'interception de la balle avec une certaine 
pr\'evision de sorte \`a y arriver en ligne de droite et plus rapidement. De 
plus, sa position de base passerait \`a une certaine distance de sa cage 
align\'ee avec une potentielle trajectoire de la balle vers la cage.

On n'est jamais parvenus \`a perfectionner l'interception de la balle et on 
compte ainsi jusqu'\`a pres\'ent un taux non n\'egligeable d'\'echec. De plus, 
comme dit pr\'ec\'edemment, une attaque \`a un n'offrait pas d'avantage \`a 
l'\'egard des \'equipes de nos camarades. Ceci a impuls\'e le 
d\'eveloppement de la d\'ej\`a mentionn\'ee mont\'ee du gardien. Lorsque son 
co\'equipier poss\`ede la balle, il monte dans le terrain pour proposer des 
possibilit\'es de passe. Son comportement avec la balle a fait aussi objet de 
modifications : c'est exactement le m\^eme que celui de l'attaquant.
On peut donc r\'esumer le comportement de notre \'equipe \`a deux joueurs en 
l'expression footballistique \enquote{la meilleure d\'efense, c'est l'attaque.}

\subsection*{D\'efenseur}
Il est \`a noter que ce joueur ne fait pas partie de l'\'equipe pr\'econis\'ee.
Il correspond \`a la derni\`ere strat\'egie impl\'ement\'ee dans cette 
cat\'egorie, \`a savoir celle d'un d\'efenseur, qui n'a par contre qu'une 
vocation d\'efensive. 
En effet, s'il a la balle, il s'en d\'efait le plus rapidement possible soit \`a 
l'aide d'une passe vers son co\'equipier si d\'emarqu\'e, soit via le 
d\'egagement le moins risqu\'e possible. 
Quant \`a son effort d\'efensif, il demeure \`a une certaine distance radial \`a 
partir de sa cage et s'approche de la balle en vue d'une interception s'il en 
suffisamment proche.

\section{Quatre joueurs}
Il s'agit de la cat\'egorie que l'on a trouv\'e la plus int\'eressante en 
raison de la difficult\'e de la gestion des espaces et la r\'epartition des 
fonctions. Tout comme pour la cat\'egorie pr\'ecedente, on compte trois 
joueurs fonctionnels que l'on rep\`ere d'ailleurs par les m\^emes noms. 

Le d\'eveloppement des strat\'egies suivantes a commenc\'e lorsque leurs pairs 
de la cat\'egorie pr\'ec\'edente \'etaient proches de leurs versions finales. 
En effet, il n'y avait que le dribble lat\'eral qui n'\'etait pas encore mis en 
place lors du d\'ebut de leur conception. 
Cette modification dans le dribble a \'et\'e ajout\'ee \`a ces strat\'egies au 
m\^eme temps que pour leurs pairs.

\subsection*{Attaquant}
Il y a deux diff\'erences en son comportement par rapport \`a son pair 
de la cat\'egorie ci-dessus et celles-ci concernent toutes les deux la 
situation o\`u il se trouve dans sa surface de r\'eparation. La premi\`ere fait 
r\'ef\'erence \`a la r\'ecup\'eration de la balle : il ne se 
pr\'ecipite plus et fait de nouveau le contr\^ole, le dribble ou la passe 
n\'ecessaire de la m\^eme mani\`ere que dans le milieu de terrain. 
La deuxi\`eme correspond \`a l'effort de marquer les adversaires sans 
marquage. En effet, les avanc\'ees adverses comportent plus de participants et 
il se r\'ev\`ele crucial de marquer les adversaires susceptibles de 
recevoir une passe pour intercepter la balle et lancer des contre-attaques 
rapides et efficaces.

\subsection*{Gardien}
Ce joueur combine le comportement de son pair de la cat\'egorie ci-dessus et de 
l'attaquant de cette cat\'egorie \`a des valeurs des param\`etres pr\`es. En 
effet, il suit les m\^emes indications que 
l'attaquant lorsqu'il contr\^ole et r\'ealise les m\^emes mouvements sans ballon 
que le d\'efenseur de l'\'equipe \`a deux joueurs.

\subsection*{D\'efenseur}
Ce d\'efenseur prend les m\^emes d\'ecisions que celui de l'\'equipe \`a 
deux joueurs, \`a l'exception de sa prise de risque lors d'une interception. 
Effectivement, si un co\'equipier se trouve dans une meilleure position pour 
intercepter la balle, i.e.\ il en est plus proche que le d\'efenseur, celui-ci 
continue de positionner radialement \`a une certaine distance de sa cage.

\newpage

\part{M\'ethodes d'optimisation}
Dans cette troisi\`eme partie, on vous d\'etaille les diff\'erents algorithmes 
appris tout au long du semestre pour l'am\'elioration des strat\'egies 
initialement propos\'ees.

\section{Recherche en grille}
La premi\`ere m\'ethode d'optimisation pre\'esent\'ee en cours a \'et\'e 
celle de la recherche en grille. Celle-ci est utilis\'ee pour l'optimisation 
d'une 
action.

Tout d'abord, on doit d\'efinir l'action \`a optimiser et le crit\`ere 
d'optimalit\'e, et rep\'erer les param\`etres associ\'es. Puis on 
obtient un ensemble de valeurs pour chaque param\`etre. 
Pour les param\`etres discrets, il suffit de prendre toutes les valeurs 
possibles, alors que pour les param\`etres continus il faudra faire en amont 
une discr\'etisation, i.e.\ on divise l'intervalle d\'elimit\'e par des bornes 
d\'efinies selon un pas de pr\'ecision. On compte ainsi des vecteurs dont les 
coordonn\'ees correspondent chacune \`a un param\`etre. On fait ensuite une 
recherche exhaustive sur tous les vecteurs obtenus : on teste chaque vecteur 
sous diff\'erents conditions environnementales et on moyenne les \'evaluations 
 du crit\`ere dans lesdites conditions. Finalement, on prend la valeur optimale 
et son vecteur associ\'e.

Il est important de remarquer que la plupart de nos param\`etres sont continus. 
Pour obtenir des valeurs plus pr\'ecises et un meilleur comportement, on a 
besoin d'un pas de discr\'etisation tr\`es petit. Ainsi, on r\'ealise qu'une 
augmentation du nombre de param\`etres \`a optimiser a une tr\`es forte 
influence sur le temps d'ex\'ecution de l'algorithme mis en place. On 
en a d\'eduit que le minuscule gain relatif nombre de param\`etres--temps 
d'ex\'ecution ne convenait pas \`a moyen terme.

\section{Algorithmes g\'en\'etiques}
Dans un deuxi\`eme temps, on est pass\'es aux algorithmes g\'en\'etiques. On en 
a mis en place une impl\'ementation pour optimiser les \'equipes compos\'ees 
d'un ou deux joueurs.

Cette classe d'algorithmes se base sur la th\'eorie de l'\'evolution. En effet, 
la notion de s\'election naturelle est le m\'ecanisme permettant de choisir des 
solutions potentielles de plus en plus meilleures.

La toute premi\`ere chose \`a faire est la d\'efinition de la fonction 
$f: \mathbb{R}^n \to \mathbb{E}$ \`a optimiser, o\`u $n$ est le nombre de 
param\`etres concern\'es. Un \'el\'ement $x=(x_1,\dotsc,x_n) \in \mathbb{R}^n$ 
est appel\'e {\itshape candidat} ou {\itshape chromosome}, et chacune de ses 
coordonn\'ees $x_i$ avec $i=1,\dotsc,n$, appel\'ee {\itshape propri\'et\'e} 
ou encore {\itshape g\`ene}, correspond \`a un param\`etre \`a optimiser. Elles 
forment le {\itshape g\'enotype}.
Autrement dit, un candidat correspond \`a une \'equipe d\'efinie par les valeurs
de ses propri\'et\'es.

Dans le cadre du projet, on a d\'efini $f(x)=(V,N,D,P,C)$ comme le 
bilan apr\`es avoir disput\'e plusieurs matches avec les m\^emes valeurs des 
param\`etres donn\'ees par $x$, i.e.\ le tuple compos\'e du 
nombre de 
victoires, matches nuls, d\'efaites, buts marqu\'es et buts encaiss\'es. La 
relation d'ordre suit les r\`egles du classement d'un tournoi de football.

Cette classe d'algorithmes comportent quatre \'etapes.

\begin{enumerate}
\item \underline{Initialisation :} Tout d'abord, il faut g\'en\'erer un 
ensemble de candidats $\{x_1,\dotsc,x_m\}$. Le but est d'obtenir une population 
initiale la plus diverse possible de sorte \`a pouvoir atteindre le plus de 
maxima relatifs. C'est pourquoi cette premi\`ere g\'en\'eration est obtenue de 
mani\`ere al\'eatoire.
\item \underline{\'Evaluation :} Ensuite, on \'evalue la fonction en 
chacun des candidats, i.e.\ on calcule $\{f(x_1),\dotsc,f(x_m)\}$.
\item \underline{S\'election :} Suivant la notion de s\'election 
naturelle, on ne conserve que les candidats avec les meilleurs r\'esultats. 
Autrement dit, on dresse un classement des \'equipes associ\'ees aux candidats 
et n'en garde qu'une partie.
\item \underline{Reproduction :} Finalement, on attribue les places 
libres aux candidats n\'es du brassage g\'en\'etique des candidats les 
plus performants. Pour ce faire, on compte sur deux m\'ethodes, \`a savoir le 
croisement et la mutation. \'Etant donn\'e deux parents, on commence par les 
cloner en deux enfants. Puis on choisit une ligne de coupe divisant le 
g\'enotype en deux parties. Ensuite, les enfants s'\'echangent une partie de 
leur g\'enotype. Le croisement s'arr\^ete \`a ce stade-l\`a, tandis que la 
mutation ajoute du bruit al\'eatoirement dans un g\`ene pour chacun 
des nouveaux chromosomes. On a d\'ecid\'e de faire une petite modification \`a 
cette \'etape-l\`a lors de l'impl\'ementation pour avoir plus de 
diversification : une minorit\'e des places libres est attrribu\'ee \`a des 
candidats g\'en\'er\'es al\'eatoirement.
\end{enumerate}

Cet algorithme \'etant it\'eratif, on recommence \`a la deuxi\`eme phase avec 
la nouvelle g\'en\'eration. Au bout d'un nombre raisonnable d'it\'erations, on 
se retrouve avec une g\'en\'eration compos\'ee de chromosomes tr\`es proches des 
maxima relatifs. On finit donc l'algorthme par prendre le candidat optimal de la 
derni\`ere population.

Cette m\'ethode a permis d'am\'eliorer nos \'equipes progressivement et les 
valeurs de tous les param\`etres mis en place ont \'et\'e d\'efinies au moyen 
de nombreuses ex\'ecutions de cette m\'ethode.

\section{Apprentissage automatique}
Dans la derni\`ere partie du semestre, on s'est concentr\'e sur l'apprentissage 
automatique. On nous a pr\'esent\'e les deux grandes familles 
d'algorithmes d'apprentissage : supervis\'es et non supervis\'es, ainsi que 
l'apprentissage par renforcement. Deux m\'ethodes ont \'et\'e impl\'ement\'ees 
lors du projet.

\subsection*{Arbres de d\'ecision (de classification)}
Il s'agit d'une technique d'apprentissage supervis\'e. Pour l'appliquer, on 
a besoin d'un {\itshape espace de repr\'esentation} $\mathcal{X}$, d'un 
ensemble d'{\itshape \'etiquettes} ou {\itshape classes} $Y$ et d'une liste de 
$m$ exemples d'apprentissage $(x^i,y^i)$, o\`u $x^i \in \mathcal{X}$, $y^i \in 
Y, i = 1,\dotsc,m$, appel\'ee {\itshape ensemble d'apprentissage}.
Le but de cette technique est de trouver une fonction $f: \mathcal{X} \to 
Y$ telle que l'on puisse pr\'edire \`a quelle classe appartient une 
future variable $x \in \mathcal{X}$.

Concr\`etement, on veut associer une certaine action \`a chaque \'etat du jeu. 
Comme le nombre d'\'etats possibles est trop grand, on utilisera une 
repr\'esentation par $n$ caract\'eristiques g\'en\'eralis\'ees qui 
pr\'ecisent la configuration du terrain. On a ainsi que $\mathcal{X} = 
\mathbb{R}^n$, avec $n$ la dimension de l'espace de repr\'esentation. De plus, 
l'ensemble d'\'etiquettes devient l'ensemble d'actions possibles $A$. On veut 
donc trouver une fonction $f: \mathbb{R}^n \to A$ qui fournit la meilleure 
action possible pour tout \'etat du jeu.

Le probl\`eme auquel on est confront\'es se r\'eduit alors \`a 
partitionner l'espace de repr\'esentation en $p=|A|$ parties 
$\mathcal{X}_1,\mathcal{X}_2,\dotsc,\mathcal{X}_p$ de sorte que 
$f([\mathcal{X}_i]) = \{ a_i \} \subset A$, $i = 1,\dotsc,p$. On utilise les 
arbres de d\'ecisions pour repr\'esenter ce partionnement. Ceci 
implique que chaque n\oe ud interne correspond \`a un test sur une des $n$ 
dimensions de $\mathcal{X}$ et que chaque feuille correspond \`a l'action \`a 
entreprendre pour l'\'etat donn\'e par le chemin suivi depuis la racine.

On nous a fourni une strat\'egie sp\'eciale \texttt{KeyboardStrategy} 
comportant les outils de base n\'ecessaires \`a la mise en place de cette 
m\'ethode, allant de la g\'en\'eration de l'ensemble d'apprentissage au 
partionnement de l'espace de repr\'esentation, en passant par l'application de 
la fonction lors des matches. 
Elle permet d'associer des touches clavier aux diff\'erentes strat\'egies de 
sorte que l'on puisse pr\'eciser la strat\'egie pr\'econis\'ee pour les 
diff\'erentes phases du jeu. Ceci conforme ce que l'on appelle l'ensemble 
d'apprentissage. Cet ensemble est trait\'e \`a l'aide des $n$ carat\'eristiques 
g\'en\'eralis\'ees d\'efinies auparavant et, \`a l'issue de ceci, on compte 
un partionnement de $\mathbb{R}^n$. Cette strat\'egie utilise alors la fonction 
$f$ lors des matches suivants, ce qui donne un comportement plus intelligent 
\`a nos joueurs.

Cette m\'ethode d'optimisation n'a pas \'et\'e utilis\'ee pour am\'eliorer la 
prise de d\'ecisions sur les actions parce que l'on a trouv\'e difficile 
l'interaction \`a travers le clavier d\`es que le nombre d'actions 
possibles d\'epasse 5.

\subsection*{Q-learning}
Cet algorithme d'apprentissage par renforcement r\'epose sur un ensemble 
d'\'etats $S$, un ensemble d'actions $A$, et une fonction $Q: S \times A \to 
\mathbb{R}$, appel\'ee {\itshape politique}, pond\'erant une paire form\'ee 
par un {\itshape \'etat} donn\'e et une {\itshape action} \`a entreprendre pour 
cet \'etat-ci. Cette pond\'eration est nomm\'ee {\itshape r\'ecompense}. La 
notion d'\'etat correspond \`a celle donn\'ee dans la section des arbres 
binaires. Il utilise aussi la notion d'{\itshape \'episode} que l'on associe 
\`a un match dans le cadre du projet. Son principe est le suivant : on apprend 
une politique comportementale qui optimise l'esp\'erance des r\'ecompenses.

La premi\`ere phase consiste \`a initialiser $Q$ \`a une valeur fixe 
arbitraire, ici 0. La deuxi\`eme phase concerne un \'episode, ou encore un 
match, et est r\'ep\'et\'ee autant de fois qu'il y a d'\'episodes. Pour un 
\'etat $s_t$ donn\'e, on choisit une action $a_t$ selon la politique $Q$ et 
observe la r\'ecompense $r_t$ associ\'ee \`a cette paire. Cette action 
g\'en\`ere un nouvel \'etat $s_{t+1}$. Il est \`a noter qu'il n'y a pas de 
choix concernant l'\'etat initial $s_0$, puisqu'il est toujours le m\^eme pour 
un match de football : ceci correspond au coup d'envoi. Ensuite, on met \`a 
jour la politique selon la formule ci-dessous.

\begin{equation*}
  Q(s_t,a_t) \gets (1-\alpha) \cdot 
  \underbrace{Q(s_t,a_t)}_{\mathclap{\text{valeur actuelle}}} + \ \alpha \cdot 
\underbrace{\Big(r_t + \gamma \cdot \max_a(Q_{t+1},a) 
  \Big)}_{\mathclap{\text{valeur   apprise}}}
\end{equation*}

Ici, $\alpha$ repr\'esente la vitesse d'apprentissage et $\gamma$ le facteur 
d'actualisation. D'un c\^ot\'e, la vitesse d'apprentissage d\'etermine 
l'\'equilibre entre exploration et exploitation. En effet, une valeur de 
0 implique l'utilisation exclusive des choix actuels, alors qu'une 
valeur 1 ne ferait que consid\'erer le dernier r\'esultat.
D'un autre c\^ot\'e, le facteur d'actualisation d\'etermine l'importance des 
r\'ecompenses futures. Une valeur de 0 ne prend en consid\'eration que les 
r\'ecompenses courantes, tandis qu'une valeur de 1 met en valeur les 
r\'ecompenses plus lointaines. On avait pr\'ecis\'e qu'un \'episode 
correspondait \`a un match ; par cons\'equent, la r\'eussite d'une succession 
d'actions d\'epend du r\'esultat final du match. De ce fait, on a int\'er\^et 
\`a retarder au maximum les r\'ecompenses. Ceci se traduit par le choix d'un 
facteur $\gamma$ proche de 1.

Juste apr\`es la mise \`a jour de la politique $Q$, on d\'etermine si le 
nouvel \'etat $s_{t+1}$ est terminal, i.e.\ qu'il correspond au coup de sifflet 
final. Dans le cas affirmatif, on passe \`a l'\'episode suivant ou on s'arr\^ete 
selon s'il y en a encore \`a faire ; on r\'eit\`ere cette phase \`a partir du 
choix d'une action pour $s_{t+1}$ dans le cas contraire.

\`A la fin de l'algorithme, on se retrouve avec une politique qui nous rend 
l'action la plus ad\'equate pour tout \'etat du jeu. \'Etant donn\'e que les 
carat\'eristiques utilis\'ees pour d\'efinir un \'etat sont continues, 
on a d\'efini des intervalles d'\'equivalence. Cela n'emp\^eche que le nombre 
total d'\'etats est \'enorme et fait donc exploser la m\'emoire. On pourrait 
mettre en place un r\'eseau de neurones pour pallier cette difficult\'e. Ceci 
n'a pas \'et\'e le cas pour ce projet et donc, bien qu'impl\'ement\'e, on 
s'est pas servi de cet algorithme pour am\'eliorer nos strat\'egies. 

\newpage

\part*{Conclusion}
...
\addcontentsline{toc}{part}{Conclusion}

\end{document}
